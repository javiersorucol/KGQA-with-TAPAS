{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "066d562d-5dac-4612-a6c0-6a73d91c2abf",
   "metadata": {},
   "source": [
    "# Results interpreter\n",
    "When performing the evaluation we decided to store the results for each question in json files on the results forlder, so we can review specific questions and see how good or wht kind of mistakes is commiting the system. To properly understand this let's check first the structure of the stored results:\n",
    "We are working with a filtered organized version of QALD9, where we only keep questions that have a sparql composed by one single triple. The questions are organized on the type of expected results and required operations to achieve them, resulting in 4 subsets: singular, multiple, boolean and aggregation. Each subset stores the results for the test and train datasets of our QALD9 subset.\n",
    "The results are a list of the questions where each question contains:\n",
    "- TP : number of true positive values in the question\n",
    "- FN : number of false negative values in the question\n",
    "- FP : number of false positive values in the question\n",
    "- correct: if the answer to the question can be considered correct or not (implies that FN=FP=0)\n",
    "- notes : Notes about the question, we annotate if any error was found when testing the question, detailing the exception\n",
    "- error : Flag to determine if the question contained an error (only two error cases: question has no english translation and the system returned an error, this is usually due to number of tokens limitations, questions with this flag set to true won't be taken into account)\n",
    "- actual_answers: list of answers returned by the system\n",
    "- expected_answers: list of answers expected by qald9 (generated from the answers key of the question)\n",
    "Here is an expample of a results file structure:\n",
    "\n",
    "{\n",
    "\n",
    "    prompt:'Optional, The prompt related to the evaluation',\n",
    "    system: 'Optional, the system related to the evaluation',\n",
    "    multiple : {\n",
    "        train_results: {\n",
    "            'question id' : {\n",
    "                TP:1,\n",
    "                FP:0,\n",
    "                FN:1,\n",
    "                correct:False,\n",
    "                notes: None\n",
    "                error: False,\n",
    "                actual_answers: ['Obama']\n",
    "                expected_answers:['Obama', 'Bush']\n",
    "                }\n",
    "            },\n",
    "        test_results: {...}\n",
    "        },\n",
    "    boolean: {...},\n",
    "    singular: {...},\n",
    "    aggregation: {...}\n",
    "\n",
    "}\n",
    "To interpret this results, we will generate the following statistics: precision, recall, f1 and percentage of correct answers for the test results, the train results and the general (test and train) for each subset.\n",
    "\n",
    "First let's define functions to achieve this..."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "28f8e8f2-371c-472c-bd34-a3bc7e142fa1",
   "metadata": {},
   "outputs": [],
   "source": [
    "# imports\n",
    "import sys\n",
    "import os\n",
    "  \n",
    "current = os.path.dirname(os.path.abspath(''))\n",
    "parent_directory = os.path.dirname(current)\n",
    "\n",
    "sys.path.append(parent_directory)\n",
    "\n",
    "from utils.Metrics_utils import get_f1, get_precision, get_recall\n",
    "from utils.Json_utils import read_json"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "87ced4bb-5918-4261-b13e-ddb81484749c",
   "metadata": {},
   "outputs": [],
   "source": [
    "def obtain_count_results(results:dict):\n",
    "    # expected input is train_results or test results of a given subset\n",
    "    # for each question\n",
    "    TP = FP = FN = correct = incorrect = 0\n",
    "    \n",
    "    for question_id, stats in results.items():\n",
    "        if not stats.get('error'):\n",
    "            TP = TP + stats.get('TP')\n",
    "            FP = FP + stats.get('FP')\n",
    "            FN = FN + stats.get('FN')\n",
    "            if stats.get('correct'):\n",
    "                correct = correct + 1\n",
    "            else:\n",
    "                incorrect = incorrect + 1\n",
    "    \n",
    "    return TP, FP, FN, correct, incorrect\n",
    "\n",
    "def print_metrics(TP, FP, FN, correct, incorrect):\n",
    "    print('TP: ', TP)\n",
    "    print('FP: ', FP)\n",
    "    print('FN: ', FN)\n",
    "    print('Correct answers: ', ((correct/(correct + incorrect))*100))\n",
    "    print('Precision: ', get_precision(TP,FP))\n",
    "    print('Recall: ', get_recall(TP,FN))\n",
    "    print('F1: ', get_f1(TP, FP, FN))\n",
    "\n",
    "def obtain_subset_results(subset:dict, subset_name:str):\n",
    "    print('Obtaining metrics for the subset: ', subset_name)\n",
    "    train_TP, train_FP, train_FN, train_correct, train_incorrect = obtain_count_results(subset.get('train_results'))\n",
    "    print('Train set results:')\n",
    "    print_metrics(train_TP, train_FP, train_FN, train_correct, train_incorrect)\n",
    "    test_TP, test_FP, test_FN, test_correct, test_incorrect = obtain_count_results(subset.get('test_results'))\n",
    "    print('Test set results:')\n",
    "    print_metrics(test_TP, test_FP, test_FN, test_correct, test_incorrect)\n",
    "    print('TOTAL results:')\n",
    "    print_metrics(train_TP + test_TP, train_FP + test_FP, train_FN + test_FN, train_correct + test_correct, train_incorrect + test_incorrect)\n",
    "    \n",
    "def interpret_results(results_filename):\n",
    "    print('Interpreting results...')\n",
    "    results_data = read_json(results_filename)\n",
    "    for key, data in results_data.items():\n",
    "        if type(data) is dict:\n",
    "            print('##############################################################')\n",
    "            obtain_subset_results(data, key)\n",
    "            print('##############################################################')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d7869ec6-06bf-4b11-9f0d-c0b2859b3bac",
   "metadata": {},
   "source": [
    "Now we are ready, let's start with the results\n",
    "## Prompting experiment\n",
    "### Prompt 1\n",
    "Prompt where the aggregation operations are asked to be performed by GPT."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "2b4cf913-4ebf-4161-a9e6-c85cae0d7d12",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Interpreting results...\n",
      "##############################################################\n",
      "Obtaining metrics for the subset:  boolean\n",
      "Train set results:\n",
      "TP:  12\n",
      "FP:  4\n",
      "FN:  9\n",
      "Correct answers:  57.14285714285714\n",
      "Precision:  0.75\n",
      "Recall:  0.5714285714285714\n",
      "F1:  0.6486486486486487\n",
      "Test set results:\n",
      "TP:  1\n",
      "FP:  0\n",
      "FN:  0\n",
      "Correct answers:  100.0\n",
      "Precision:  1.0\n",
      "Recall:  1.0\n",
      "F1:  1.0\n",
      "TOTAL results:\n",
      "TP:  13\n",
      "FP:  4\n",
      "FN:  9\n",
      "Correct answers:  59.09090909090909\n",
      "Precision:  0.7647058823529411\n",
      "Recall:  0.5909090909090909\n",
      "F1:  0.6666666666666666\n",
      "##############################################################\n",
      "##############################################################\n",
      "Obtaining metrics for the subset:  aggregation\n",
      "Train set results:\n",
      "TP:  3\n",
      "FP:  36\n",
      "FN:  9\n",
      "Correct answers:  25.0\n",
      "Precision:  0.07692307692307693\n",
      "Recall:  0.25\n",
      "F1:  0.11764705882352941\n",
      "Test set results:\n",
      "TP:  0\n",
      "FP:  11\n",
      "FN:  2\n",
      "Correct answers:  0.0\n",
      "Precision:  0.0\n",
      "Recall:  0.0\n",
      "F1:  0.0\n",
      "TOTAL results:\n",
      "TP:  3\n",
      "FP:  47\n",
      "FN:  11\n",
      "Correct answers:  21.428571428571427\n",
      "Precision:  0.06\n",
      "Recall:  0.21428571428571427\n",
      "F1:  0.09375\n",
      "##############################################################\n",
      "##############################################################\n",
      "Obtaining metrics for the subset:  singular\n",
      "Train set results:\n",
      "TP:  73\n",
      "FP:  21\n",
      "FN:  16\n",
      "Correct answers:  76.40449438202246\n",
      "Precision:  0.776595744680851\n",
      "Recall:  0.8202247191011236\n",
      "F1:  0.7978142076502732\n",
      "Test set results:\n",
      "TP:  27\n",
      "FP:  3\n",
      "FN:  8\n",
      "Correct answers:  70.96774193548387\n",
      "Precision:  0.9\n",
      "Recall:  0.7714285714285715\n",
      "F1:  0.8307692307692308\n",
      "TOTAL results:\n",
      "TP:  100\n",
      "FP:  24\n",
      "FN:  24\n",
      "Correct answers:  75.0\n",
      "Precision:  0.8064516129032258\n",
      "Recall:  0.8064516129032258\n",
      "F1:  0.8064516129032258\n",
      "##############################################################\n",
      "##############################################################\n",
      "Obtaining metrics for the subset:  multiple\n",
      "Train set results:\n",
      "TP:  152\n",
      "FP:  22\n",
      "FN:  6008\n",
      "Correct answers:  52.27272727272727\n",
      "Precision:  0.8735632183908046\n",
      "Recall:  0.024675324675324677\n",
      "F1:  0.04799494790022103\n",
      "Test set results:\n",
      "TP:  76\n",
      "FP:  34\n",
      "FN:  352\n",
      "Correct answers:  47.82608695652174\n",
      "Precision:  0.6909090909090909\n",
      "Recall:  0.17757009345794392\n",
      "F1:  0.2825278810408922\n",
      "TOTAL results:\n",
      "TP:  228\n",
      "FP:  56\n",
      "FN:  6360\n",
      "Correct answers:  50.74626865671642\n",
      "Precision:  0.8028169014084507\n",
      "Recall:  0.03460837887067395\n",
      "F1:  0.06635622817229336\n",
      "##############################################################\n"
     ]
    }
   ],
   "source": [
    "interpret_results('../results/prompt_1_gpt_operations_results.json')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fc08cb74-1c29-4be0-8736-edf4d6c157e1",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
